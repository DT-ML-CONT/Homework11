{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework11\n",
    "\n",
    "Exercises with text processing and NLP modeling\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Understand similarities and differences between the processes of working with text, images and tabular data\n",
    "- Practice with different methods of encoding and modeling text data\n",
    "- See different methods for extracting information or patterns from text datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/text_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from data_utils import display_silhouette_plots, object_from_json_url\n",
    "from text_utils import get_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell it's gonna be a good homework from the number of imports.\n",
    "# üôÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have protein, need seasoning\n",
    "\n",
    "Let's create a model to help us season our foods. In the end, what we want is a model that receives a short list of ingredients and returns a list of seasonings or complementary ingredients for our original ingredients list.\n",
    "\n",
    "In order to do that we need a dataset of recipes. We'll load that into a text dataset where each recipe is a document and the ingredients are our document *tokens*.\n",
    "\n",
    "Let's take a look at the recipe dataset and become familiar with the data and how it's organized.\n",
    "\n",
    "We'll load our recipes and do a bit of exploratory data analysis to look for patterns first to see if this kind of modeling makes any sense.\n",
    "\n",
    "### Load Data\n",
    "\n",
    "Here's our dataset. Let's load it into an object for inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"https://raw.githubusercontent.com/PSAM-5020-2025F-A/5020-utils/refs/heads/main/datasets/text/recipes/recipes_min16.json\"\n",
    "recipes_obj = object_from_json_url(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Data\n",
    "\n",
    "How's the data organized?\n",
    "\n",
    "How many recipes do we have?\n",
    "\n",
    "Do all recipes have the same number of ingredients?\n",
    "\n",
    "Anything else stand out about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Look at Data here\n",
    "recipes_obj[:3]    # shows first few recipes\n",
    "# TODO: How many recipes\n",
    "num_recipes = len(recipes_obj)\n",
    "num_recipes\n",
    "# TODO: How many ingredients do the shortest and longest recipes have?\n",
    "ingredient_lengths = [len(r[\"ingredients\"]) for r in recipes_obj]\n",
    "min_len = min(ingredient_lengths)\n",
    "max_len = max(ingredient_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Features\n",
    "\n",
    "Our dataset doesn't really have to be a `DataFrame` here. It can, but it doesn't have to be.\n",
    "\n",
    "Each recipe right now is described as a list of ingredients, but what we really want is a list of *sentences*, where each *sentence* is a Python `string` with all of the ingredients for a given recipe.\n",
    "\n",
    "Instead of:<br>```[\"salt\", \"baking soda\", \"water\", \"mushroom\"]```,\n",
    "\n",
    "we want:<br>```\"salt baking soda water mushroom\"```\n",
    "\n",
    "The `join()` function might help.\n",
    "\n",
    "Another thing to consider is wether we want to do anything special about multi-word ingredients, like *baking soda*.\n",
    "\n",
    "Do we want to let our vectorizer (spoiler) split that into two tokens, or do we want to guarantee that *baking* and *soda* always stay together? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: turn list of objects into list of strings\n",
    "recipe_texts = [\" \".join(recipe[\"ingredients\"]) for recipe in recipes_obj]\n",
    "\n",
    "# Inspect first few results\n",
    "recipe_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Data\n",
    "\n",
    "The fun part.\n",
    "\n",
    "Let's vectorize our list of ingredient strings into a sparse document matrix using `CountVectorizer` or `TfidfVectorizer`.\n",
    "\n",
    "The resulting matrix will have one row for each recipe, and the columns will encode the ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Vectorize ingredients from our recipe list\n",
    "vectorizer = TfidfVectorizer()   # you could also use CountVectorizer()\n",
    "X = vectorizer.fit_transform(recipe_texts)\n",
    "\n",
    "# TODO: How many words are in our vocabulary?\n",
    "vocab_size = len(vectorizer.vocabulary_)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Data\n",
    "\n",
    "Now that we have our recipes/documents vectorized we can study them a little bit, and look for patterns.\n",
    "\n",
    "What happens if we cluster our recipes ? What do the cluster centers represent ?\n",
    "\n",
    "When might this be useful ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: cluster recipes\n",
    "\n",
    "# number of clusters\n",
    "k = 6\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Cluster assignments for each recipe\n",
    "recipe_clusters = kmeans.labels_\n",
    "\n",
    "recipe_clusters[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Centers\n",
    "\n",
    "Use the `get_top_words()` function to decode the `cluster_centers` back into ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Look at cluster centers\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(centers.shape[0]):\n",
    "    print(f\"\\n=== Cluster {i} ===\")\n",
    "    \n",
    "    center_vec = centers[i].reshape(1, -1)\n",
    "    \n",
    "    top_words = get_top_words(center_vec, feature_names, 10)\n",
    "    print(top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "What do these cluster centers represent ?<br>\n",
    "Is there anything interesting about recipe cluster centers ?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">\n",
    "Each cluster center shows the ‚Äútypical ingredients‚Äù for that group of recipes. \n",
    "The top ingredients tell us what kind of recipes are in each cluster:\n",
    "\n",
    "- Cluster 0: lime, cilantro, chili ‚Üí Mexican or Latin dishes\n",
    "- Cluster 1: cheese, basil, garlic ‚Üí Italian dishes like pasta or pizza\n",
    "- Cluster 2: soy sauce, sesame, ginger ‚Üí Asian stir-fry recipes\n",
    "- Cluster 3: olive oil, tomatoes, parsley ‚Üí Mediterranean dishes\n",
    "- Cluster 4: flour, sugar, butter, vanilla ‚Üí baked goods or desserts\n",
    "- Cluster 5: cheese, tortilla, salsa ‚Üí Mexican/Tex-Mex dishes\n",
    "\n",
    "Interesting things:\n",
    "\n",
    "- Some common ingredients (like garlic, salt, pepper) appear in many clusters.  \n",
    "- Clustering shows that recipes with similar ingredients naturally group together, even without looking at instructions.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Clusters\n",
    "\n",
    "Let's plot our clusters to see if we have to adjust any of the clustering parameters.\n",
    "\n",
    "Since we can't plot in $500$ dimensions, we should use `TruncatedSVD` to look at our clusters in $2D$ and $3D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: TruncatedSVD to reduce the dimensions of our feature space\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "# TODO: plot clusters\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "colors = cm.get_cmap('tab10', k)  # k = number of clusters\n",
    "for i in range(k):\n",
    "    plt.scatter(\n",
    "        X_reduced[recipe_clusters==i, 0], \n",
    "        X_reduced[recipe_clusters==i, 1], \n",
    "        label=f'Cluster {i}', \n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"SVD Component 1\")\n",
    "plt.ylabel(\"SVD Component 2\")\n",
    "plt.title(\"Recipe Clusters in 2D\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "What does the graph look like ?<br>\n",
    "Are the clusters well-separated ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">\n",
    "The plot shows all recipes in 2D after simplifying their ingredient lists using TruncatedSVD.\n",
    "Each color is a different cluster.\n",
    "\n",
    "Observations:\n",
    "\n",
    "Cluster 4 (purple) is quite separate from the others, so these recipes are likely very different (maybe desserts or baked goods).\n",
    "\n",
    "The other clusters (0, 1, 2, 3, 5) overlap a lot, which means these recipes share many ingredients and aren‚Äôt clearly separated in 2D.\n",
    "\n",
    "Some clusters stand out more than others, but many recipes have ingredients in common, making it hard to fully separate them.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "The clustering groups recipes in a meaningful way, but since ingredients overlap, the clusters aren‚Äôt completely distinct.\n",
    "\n",
    "Using more clusters or more dimensions might make the separation clearer.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Silhouette Plots\n",
    "\n",
    "We can also check the quality of our clustering by looking at the silhouette plots that we get from calling:<br>\n",
    "`display_silhouette_plots(vectors, clusters)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_silhouette_plots(X, recipe_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "How many clusters did you end up with ?<br>\n",
    "How do they look ?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">\n",
    "I used 6 clusters total.\n",
    "\n",
    "Clusters 4 and 5 look the strongest because their silhouette scores are mostly high. This means the recipes in those groups are very similar to each other.\n",
    "\n",
    "The other clusters (0, 1, 2, 3) have lower scores and more variation, so those groups are not as cleanly separated.\n",
    "\n",
    "A few recipes go slightly below 0, which means they might fit better in another cluster.\n",
    "\n",
    "Overall:\n",
    "The clustering works, but only some groups are very clear. Many recipes share similar ingredients, so the separation isn‚Äôt perfect.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe Completion\n",
    "\n",
    "Ok. On to the main event.\n",
    "\n",
    "Let's create some recipes.\n",
    "\n",
    "We'll do this using a technique similar to what is used for movie/product recommendations. Given an initial set of ingredients, we'll look at recipes that have similar ingredients and \"recommend\" additional ingredients.\n",
    "\n",
    "We already have all of the recipes in our dataset encoded as `tf-idf` vectors. The rest of our algorithm will be something like:\n",
    "1. Start with an initial set of ingredients\n",
    "2. Encode ingredients\n",
    "3. Find a set of recipes that are similar to our list of ingredients\n",
    "4. Find common ingredients that are in the similar recipes, but not in our list of ingredients\n",
    "5. Pick representative ingredient to add to recipe\n",
    "6. Repeat\n",
    "\n",
    "Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial list of ingredients\n",
    "\n",
    "This is just a string with ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_seed_str = \"tofu\" # feel free to change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encode ingredients\n",
    "\n",
    "Transform the string into a `tf-idf` vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: transform string into sparse vector\n",
    "\n",
    "recipe_seed_vct = vectorizer.transform([recipe_seed_str])\n",
    "recipe_seed_vct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find similar recipes\n",
    "\n",
    "The meat of the algorithm. No pun intended.\n",
    "\n",
    "In order to find similar recipes, we'll first calculate the distance between our current list of ingredients and all recipes in our dataset.\n",
    "\n",
    "We can start with euclidean distance and later try other kinds, but the overall processing will be the same:\n",
    "\n",
    "1. Start with an empty list to store distances\n",
    "2. Loop over the `tf-idf` recipe vectors and for each vector:\n",
    "   1. Subtract the ingredient list\n",
    "   2. Square the difference (to square a sparse matrix `A`, use `A.multiply(A)`)\n",
    "   3. Sum the terms of the result\n",
    "   4. Take the square root of the sum\n",
    "   5. Append to distance list\n",
    "3. Find the indices of the smallest distances (this operation is called `argsort` and will give us the indices of the recipes that are most similar to our list of ingredients)\n",
    "4. Check the recipes to see if they are indeed similar (`inverse_transform()` the vectors at the indices calculated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argsort a list (get sequence of indices that would sort the list)\n",
    "# https://stackoverflow.com/a/3382369\n",
    "def argsort(L, reverse=False):\n",
    "  return sorted(range(len(L)), key=L.__getitem__, reverse=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# TODO: list to keep distances\n",
    "recipe_dists = []\n",
    "\n",
    "# TODO: loop over vectors and append euclidean distances to list\n",
    "for v in X:                    # X = all recipe tf-idf vectors\n",
    "    diff = v - recipe_seed_vct\n",
    "    sq = diff.multiply(diff)  # elementwise square (sparse-safe)\n",
    "    dist = np.sqrt(sq.sum())  # Euclidean distance\n",
    "    recipe_dists.append(dist)\n",
    "    \n",
    "# TODO: argsort list of distances to find indices of similar recipes\n",
    "similar_idxs = argsort(recipe_dists)\n",
    "\n",
    "# TODO: check first 4 recipes\n",
    "for i in similar_idxs[:4]:\n",
    "    print(vectorizer.inverse_transform(X[i])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find ingredients to recommend\n",
    "\n",
    "We have a way to get a set of similar recipes with similar ingredients, and now want to find a *meaningful*, or *representative*, ingredient to add to our ingredients list.\n",
    "\n",
    "Let's consider ingredients in the $16$ most similar recipes. What we are trying to do is find an ingredient that is in a lot of these recipes, but not yet in our list of ingredients.\n",
    "\n",
    "There are many possible ways of doing this. We could count the number of times different ingredients show up in these $16$ recipes using Python dictionaries and/or sets, but what we're trying to do here is very similar to what a `TfidfVectorizer` does: calculate relative importance of terms in a series of documents.\n",
    "\n",
    "Let's re-encode these $16$ recipes using their own separate `TfidfVectorizer`, then sum the importance of each ingredient and look at ingredients with the highest importance scores.\n",
    "\n",
    "We could re-use the vectors/scores from the original `TfidfVectorizer`, but they're gonna be influenced by the relative frequencies of all of the ingredients that showed up in all of the recipes. Using a separate vectorizer is a little bit more precise.\n",
    "\n",
    "The steps we need to take are:\n",
    "\n",
    "1. Separate the $16$ recipes most similar to our list of ingredients\n",
    "   1. We have lots of representations of our recipes, but `recipes` (list of strings) might be the easiest one to use here\n",
    "2. Create a new `TfidfVectorizer` and encode the $16$ recipes\n",
    "3. Sum the resulting vectors to get overall importance scores for each ingredient/token\n",
    "4. Convert resulting vector to a list using `A.tolist()[0]`\n",
    "5. `argsort` the importance scores to get sequence of ingredient indices ordered from most to least important\n",
    "6. Find the most important ingredient that isn't on the ingredient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from data_utils import object_from_json_url\n",
    "\n",
    "# 1. Load recipes dataset\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/PSAM-5020-2025F-A/5020-utils/refs/heads/main/datasets/text/recipes/recipes_min16.json\"\n",
    "recipes_obj = object_from_json_url(DATA_PATH)\n",
    "\n",
    "# 2. Convert each recipe's list of ingredients into a single string\n",
    "recipe_texts = [\" \".join(recipe[\"ingredients\"]) for recipe in recipes_obj]\n",
    "\n",
    "# 3. Convert all recipes to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(recipe_texts)\n",
    "\n",
    "# 4. Seed ingredient\n",
    "recipe_seed_str = \"tofu\"  # change this to try a different ingredient\n",
    "\n",
    "# 5. Transform seed into tf-idf vector\n",
    "recipe_seed_vct = vectorizer.transform([recipe_seed_str])\n",
    "\n",
    "# 6. Compute Euclidean distances to all recipes\n",
    "recipe_dists = []\n",
    "for v in X:  # X = tf-idf vectors of all recipes\n",
    "    diff = v - recipe_seed_vct\n",
    "    sq = diff.multiply(diff)\n",
    "    dist = np.sqrt(sq.sum())\n",
    "    recipe_dists.append(dist)\n",
    "\n",
    "# 7. Define argsort function\n",
    "def argsort(L, reverse=False):\n",
    "    return sorted(range(len(L)), key=L.__getitem__, reverse=reverse)\n",
    "\n",
    "# 8. Get indices of recipes most similar to the seed\n",
    "similar_idxs = argsort(recipe_dists)\n",
    "\n",
    "# TODO: Get 16 most similar recipes\n",
    "k = 16\n",
    "top_recipes = [recipe_texts[i] for i in similar_idxs[:k]]\n",
    "\n",
    "# TODO: Encode the 16 recipes\n",
    "local_vectorizer = TfidfVectorizer()\n",
    "local_vcts = local_vectorizer.fit_transform(top_recipes)\n",
    "\n",
    "# TODO: Sum the recipe vectors by column to get ingredient importance scores\n",
    "ingredient_scores = local_vcts.sum(axis=0).tolist()[0]\n",
    "\n",
    "# TODO: argsort the importance scores\n",
    "sorted_idx = argsort(ingredient_scores, reverse=True)\n",
    "\n",
    "# TODO: Find most important ingredient not yet on the list of ingredients\n",
    "current_ingredients = set(recipe_seed_str.split())\n",
    "for idx in sorted_idx:\n",
    "    ing = local_vectorizer.get_feature_names_out()[idx]\n",
    "    if ing not in current_ingredients:\n",
    "        print(\"Recommended ingredient:\", ing)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add ingredient to recipe\n",
    "\n",
    "This is simply adding a word to `recipe_seed_str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: add the first important ingredient to list of ingredients\n",
    "# Add the recommended ingredient to the seed list\n",
    "recipe_seed_str += \" \" + ing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Repeat (Optional)\n",
    "\n",
    "Now we can repeat this process until we get an empty list of important ingredients: \n",
    "1. Encode current recipe\n",
    "2. Find similar recipes\n",
    "3. Find important ingredients\n",
    "4. Add important ingredient\n",
    "\n",
    "Might be helpful to define a couple of functions, like `find_similar_recipes()` and `find_important_ingredients()`...\n",
    "\n",
    "Only do this step if you're really curious about experimenting with generating unconventional ingredient lists. It's not going to be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Create find_similar_recipes(ingredients, recipes, vectorizer)\n",
    "\n",
    "# TODO: Create find_important_ingredients(recipes)\n",
    "\n",
    "# TODO: Create recipe by repeating calls to find_similar_recipes() and find_important_ingredients()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
